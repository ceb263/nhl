{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRS3s3MZpa4aZfMSFDJXiP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceb263/nhl/blob/main/xG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMLSYBy7rtF4"
      },
      "source": [
        "# Imports and input data\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_rows', 150)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftMGJRmqdkIC",
        "outputId": "653196ed-db72-4de8-9a3d-5ee049453032"
      },
      "source": [
        "!unzip data_2012-2019.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data_2012-2019.zip\n",
            "  inflating: pbp_2019.pkl            \n",
            "  inflating: __MACOSX/._pbp_2019.pkl  \n",
            "  inflating: pbp_2012.pkl            \n",
            "  inflating: __MACOSX/._pbp_2012.pkl  \n",
            "  inflating: pbp_2013.pkl            \n",
            "  inflating: __MACOSX/._pbp_2013.pkl  \n",
            "  inflating: pbp_2014.pkl            \n",
            "  inflating: __MACOSX/._pbp_2014.pkl  \n",
            "  inflating: pbp_2015.pkl            \n",
            "  inflating: __MACOSX/._pbp_2015.pkl  \n",
            "  inflating: pbp_2016.pkl            \n",
            "  inflating: __MACOSX/._pbp_2016.pkl  \n",
            "  inflating: pbp_2017.pkl            \n",
            "  inflating: __MACOSX/._pbp_2017.pkl  \n",
            "  inflating: pbp_2018.pkl            \n",
            "  inflating: __MACOSX/._pbp_2018.pkl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmTRl1ZYt7B4"
      },
      "source": [
        "# read data\n",
        "plays = pd.read_pickle('pbp_2012.pkl')\n",
        "plays['Season'] = 2012\n",
        "plays2013 = pd.read_pickle('pbp_2013.pkl')\n",
        "plays2013['Season'] = 2013\n",
        "plays2014 = pd.read_pickle('pbp_2014.pkl')\n",
        "plays2014['Season'] = 2014\n",
        "plays2015 = pd.read_pickle('pbp_2015.pkl')\n",
        "plays2015['Season'] = 2015\n",
        "plays2016 = pd.read_pickle('pbp_2016.pkl')\n",
        "plays2016['Season'] = 2016\n",
        "plays2017 = pd.read_pickle('pbp_2017.pkl')\n",
        "plays2017['Season'] = 2017\n",
        "plays2018 = pd.read_pickle('pbp_2018.pkl')\n",
        "plays2018['Season'] = 2018\n",
        "plays2019 = pd.read_pickle('pbp_2019.pkl')\n",
        "plays2019['Season'] = 2019\n",
        "plays = pd.concat([plays, plays2013], ignore_index=True)\n",
        "plays = pd.concat([plays, plays2014], ignore_index=True)\n",
        "plays = pd.concat([plays, plays2015], ignore_index=True)\n",
        "plays = pd.concat([plays, plays2016], ignore_index=True)\n",
        "plays = pd.concat([plays, plays2017], ignore_index=True)\n",
        "plays = pd.concat([plays, plays2018], ignore_index=True)\n",
        "plays = pd.concat([plays, plays2019], ignore_index=True)\n",
        "\n",
        "del plays2013, plays2014, plays2015, plays2016, plays2017, plays2018, plays2019"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TXg11RbuaYP"
      },
      "source": [
        "# data preprocessing\n",
        "def preprocess_plays(df):\n",
        "    # remove null location data\n",
        "    df = df.loc[~df['xC'].isnull()]\n",
        "    df = df.loc[~df['yC'].isnull()]\n",
        "\n",
        "    # get previous event time and location\n",
        "    df = df.sort_values(by=['Game_Id','Period','Seconds_Elapsed'])\n",
        "    df['prev_Game_Id'] = df['Game_Id'].shift(1)\n",
        "    df['prev_Period'] = df['Period'].shift(1)\n",
        "    df['keepPrev'] = ((df['prev_Game_Id']==df['Game_Id']) & (df['prev_Period']==df['Period'])).astype(int)\n",
        "    df['prev_Event'] = df['Event'].shift(1)\n",
        "    df['prev_Seconds_Elapsed'] = df['Seconds_Elapsed'].shift(1)\n",
        "    df['prev_xC'] = df['xC'].shift(1)\n",
        "    df['prev_yC'] = df['yC'].shift(1)\n",
        "    df.at[df['keepPrev']==0, ['prev_Event','prev_Seconds_Elapsed','prev_xC','prev_yC']] = np.NaN\n",
        "    df = df.loc[~df['prev_Event'].isnull()]\n",
        "\n",
        "    # get time elapsed, and distance from previous event\n",
        "    df['timeSincePrev'] = df['Seconds_Elapsed'] - df['prev_Seconds_Elapsed']\n",
        "    df['distanceSincePrev'] = np.sqrt(np.square(df['xC']-df['prev_xC']) + np.square(df['yC']-df['prev_yC']))\n",
        "    df['yDistanceSincePrev'] = np.abs(df['yC'] - df['prev_yC'])\n",
        "\n",
        "    # remove invalid data\n",
        "    df = df.loc[(df['timeSincePrev']>0) | (df['timeSincePrev'].isnull())]\n",
        "\n",
        "    # filter for only shots and shot attempts\n",
        "    df = df.loc[df['Event'].isin(['BLOCK','MISS','SHOT','GOAL'])]\n",
        "\n",
        "    # filter for only most common game states\n",
        "    df = df.loc[df['Strength'].isin(['5x5','4x5','3x5','5x4','4x4','5x3','4x3','6x5','5x6','3x4','3x3'])]\n",
        "\n",
        "    # get previous shot time and location, and then calculate derived metrics\n",
        "    df['prevShot_Game_Id'] = df['Game_Id'].shift(1)\n",
        "    df['prevShot_Period'] = df['Period'].shift(1)\n",
        "    df['keepPrevShot'] = ((df['prevShot_Game_Id']==df['Game_Id']) & (df['prevShot_Period']==df['Period'])).astype(int)\n",
        "    df['prevShot_Seconds_Elapsed'] = df['Seconds_Elapsed'].shift(1)\n",
        "    df['prevShot_xC'] = df['xC'].shift(1)\n",
        "    df['prevShot_yC'] = df['yC'].shift(1)\n",
        "    df['prevShot_Ev_Team'] = df['Ev_Team'].shift(1)\n",
        "    df['prevShot_sameTeam'] = (df['prevShot_Ev_Team']==df['Ev_Team']).astype(int)\n",
        "    df.at[df['keepPrevShot']==0, ['prevShot_Seconds_Elapsed','prevShot_xC','prevShot_yC','prevShot_Ev_Team']] = np.NaN\n",
        "    df['timeSincePrevShot'] = df['Seconds_Elapsed'] - df['prevShot_Seconds_Elapsed']\n",
        "    df['distanceSincePrevShot'] = np.sqrt(np.square(df['xC']-df['prevShot_xC']) + np.square(df['yC']-df['prevShot_yC']))\n",
        "    df['yDistanceSincePrevShot'] = np.abs(df['yC'] - df['prevShot_yC'])\n",
        "\n",
        "    # adjust shot locations so everything is on the same side of the ice\n",
        "    # TODO this isn't quite right - shots from the D zone (into an empty net, for example), will not be adjusted correctly\n",
        "    df['loc_adjust_factor'] = (((df['xC']>0).astype(int).astype(float)) - 0.5) * 2\n",
        "    df['x_adj'] = df['xC']*df['loc_adjust_factor']\n",
        "    df['y_adj'] = df['yC']*df['loc_adjust_factor']\n",
        "    df['prev_loc_adjust_factor'] = (((df['prev_xC']>0).astype(int).astype(float)) - 0.5) * 2\n",
        "    df['prev_x_adj'] = df['prev_xC']*df['prev_loc_adjust_factor']\n",
        "    df['prev_y_adj'] = df['prev_yC']*df['prev_loc_adjust_factor']\n",
        "    df['prevShot_loc_adjust_factor'] = (((df['prevShot_xC']>0).astype(int).astype(float)) - 0.5) * 2\n",
        "    df['prevShot_x_adj'] = df['prevShot_xC']*df['prevShot_loc_adjust_factor']\n",
        "    df['prevShot_y_adj'] = df['prevShot_yC']*df['prevShot_loc_adjust_factor']\n",
        "\n",
        "    # fill nulls\n",
        "    df[['prevShot_Seconds_Elapsed','prevShot_yC','prevShot_y_adj','prevShot_xC']] = \\\n",
        "        df[['prevShot_Seconds_Elapsed','prevShot_yC','prevShot_y_adj','prevShot_xC']].fillna(0)\n",
        "    df[['prevShot_x_adj','distanceSincePrevShot','yDistanceSincePrevShot']] = df[['prevShot_x_adj','distanceSincePrevShot','yDistanceSincePrevShot']].fillna(-1)\n",
        "    df[['timeSincePrevShot']] = df[['timeSincePrevShot']].fillna(1200)\n",
        "\n",
        "    # fix time since prev shot if prev shot was in another period\n",
        "    df.at[df['timeSincePrevShot']<0, 'timeSincePrevShot'] = 1200\n",
        "\n",
        "    # adjust score to be score for and against, instead of home and away\n",
        "    df['homeTeamShot'] = (df['Home_Team']==df['Ev_Team']).astype(int)\n",
        "    df['scoreFor'] = (df['Home_Score']*df['homeTeamShot']) + (df['Away_Score']*(1-df['homeTeamShot']))\n",
        "    df['scoreAgainst'] = (df['Away_Score']*df['homeTeamShot']) + (df['Home_Score']*(1-df['homeTeamShot']))\n",
        "    df['scoreDiff'] = df['scoreFor'] - df['scoreAgainst']\n",
        "\n",
        "    # add target variable\n",
        "    df['goal'] = (df['Event']=='GOAL').astype(int)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LlKy5vD-IFv"
      },
      "source": [
        "shots = preprocess_plays(plays)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WvRQZoB6uTLm",
        "outputId": "3ed536f9-277e-4339-9959-c98bf6c99a83"
      },
      "source": [
        "# select feature and target columns\n",
        "X = shots[['Period','Seconds_Elapsed','scoreFor','scoreAgainst','scoreDiff','xC','yC','prev_Seconds_Elapsed','prev_xC','prev_yC','timeSincePrev','distanceSincePrev',\n",
        "           'yDistanceSincePrev','prevShot_Seconds_Elapsed','prevShot_xC','prevShot_yC','prevShot_sameTeam','timeSincePrevShot','distanceSincePrevShot',\n",
        "           'yDistanceSincePrevShot','x_adj','y_adj','prev_x_adj','prev_y_adj','prevShot_x_adj','prevShot_y_adj','Strength','Ev_Zone','Type','prev_Event','goal','Season']]\n",
        "y = shots[['goal','Season']]\n",
        "\n",
        "# train/test split\n",
        "#[X_train, X_test, y_train, y_test] = train_test_split(X, y, test_size=0.3, random_state=26)\n",
        "X_train = X.loc[X['Season']!=2019]\n",
        "X_test = X.loc[X['Season']==2019]\n",
        "y_train = y.loc[y['Season']!=2019]['goal'].values\n",
        "y_test = y.loc[y['Season']==2019]['goal'].values\n",
        "\n",
        "# categorical feature encodings\n",
        "mean_codes_strength = X_train.groupby(['Strength'])['goal'].mean().to_dict()\n",
        "mean_codes_zone = X_train.groupby(['Ev_Zone'])['goal'].mean().to_dict()\n",
        "mean_codes_type = X_train.groupby(['Type'])['goal'].mean().to_dict()\n",
        "mean_codes_prevEvent = X_train.groupby(['prev_Event'])['goal'].mean().to_dict()\n",
        "\n",
        "tot_count = X_train['goal'].count()\n",
        "count_codes_strength = (X_train.groupby(['Strength'])['goal'].count()/tot_count).to_dict()\n",
        "count_codes_zone = (X_train.groupby(['Ev_Zone'])['goal'].count()/tot_count).to_dict()\n",
        "count_codes_type = (X_train.groupby(['Type'])['goal'].count()/tot_count).to_dict()\n",
        "count_codes_prevEvent = (X_train.groupby(['prev_Event'])['goal'].count()/tot_count).to_dict()\n",
        "\n",
        "X_train['Strength_meanEnc'] = X_train['Strength'].map(mean_codes_strength)\n",
        "X_test['Strength_meanEnc'] = X_test['Strength'].map(mean_codes_strength)\n",
        "X_train['Ev_Zone_meanEnc'] = X_train['Ev_Zone'].map(mean_codes_zone)\n",
        "X_test['Ev_Zone_meanEnc'] = X_test['Ev_Zone'].map(mean_codes_zone)\n",
        "X_train['Type_meanEnc'] = X_train['Type'].map(mean_codes_type)\n",
        "X_test['Type_meanEnc'] = X_test['Type'].map(mean_codes_type)\n",
        "X_train['prev_Event_meanEnc'] = X_train['prev_Event'].map(mean_codes_prevEvent)\n",
        "X_test['prev_Event_meanEnc'] = X_test['prev_Event'].map(mean_codes_prevEvent)\n",
        "X_train['Strength_countEnc'] = X_train['Strength'].map(count_codes_strength)\n",
        "X_test['Strength_countEnc'] = X_test['Strength'].map(count_codes_strength)\n",
        "X_train['Ev_Zone_countEnc'] = X_train['Ev_Zone'].map(count_codes_zone)\n",
        "X_test['Ev_Zone_countEnc'] = X_test['Ev_Zone'].map(count_codes_zone)\n",
        "X_train['Type_countEnc'] = X_train['Type'].map(count_codes_type)\n",
        "X_test['Type_countEnc'] = X_test['Type'].map(count_codes_type)\n",
        "X_train['prev_Event_countEnc'] = X_train['prev_Event'].map(count_codes_prevEvent)\n",
        "X_test['prev_Event_countEnc'] = X_test['prev_Event'].map(count_codes_prevEvent)\n",
        "\n",
        "X_train = X_train.drop(['goal','Strength','Ev_Zone','Type','prev_Event','Season'],1).values\n",
        "X_test = X_test.drop(['goal','Strength','Ev_Zone','Type','prev_Event','Season'],1).values\n",
        "\n",
        "'''\n",
        "strengthOneHot = LabelBinarizer()\n",
        "zoneOneHot = LabelBinarizer()\n",
        "typeOneHot = LabelBinarizer()\n",
        "prevEventOneHot = LabelBinarizer()\n",
        "X = np.concatenate((X, strengthOneHot.fit_transform(shots['Strength']), zoneOneHot.fit_transform(shots['Ev_Zone']), typeOneHot.fit_transform(shots['Type']), \n",
        "                   prevEventOneHot.fit_transform(shots['prev_Event'])), axis=1)\n",
        "'''\n",
        "\n",
        "# apply scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLecsfqrQk9W"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8q4DRF0tUCf"
      },
      "source": [
        "[_, X_subsample, _, y_subsample] = train_test_split(X_train, y_train, test_size=0.01, random_state=26)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAzxlupkv_e7",
        "outputId": "1d07bac1-306b-4bb7-eac2-92d9eb20c283"
      },
      "source": [
        "X_subsample.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8292, 34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "y_yuJ80xs-_G",
        "outputId": "fa91665f-d332-47e5-9c8e-98a7481d3aaf"
      },
      "source": [
        "model_test = KNeighborsClassifier()\n",
        "\n",
        "param_dist = {\n",
        "    'n_neighbors' : [300,500]\n",
        "}\n",
        "\n",
        "random_search = GridSearchCV(model_test, param_dist, scoring=['neg_log_loss','roc_auc'], refit='neg_log_loss', cv=3, return_train_score=True)\n",
        "random_search.fit(X_subsample, y_subsample)\n",
        "\n",
        "report_cols = ['mean_test_neg_log_loss','mean_test_roc_auc']+['param_'+param for param in param_dist]\n",
        "report = pd.DataFrame(random_search.cv_results_)[report_cols].sort_values(by='mean_test_neg_log_loss', ascending=False)\n",
        "report"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_test_neg_log_loss</th>\n",
              "      <th>mean_test_roc_auc</th>\n",
              "      <th>param_n_neighbors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.169279</td>\n",
              "      <td>0.740524</td>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.171191</td>\n",
              "      <td>0.734494</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_test_neg_log_loss  mean_test_roc_auc param_n_neighbors\n",
              "0               -0.169279           0.740524               300\n",
              "1               -0.171191           0.734494               500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjskfGCRQp6a"
      },
      "source": [
        "# Selected Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck2v8xAJOaQF",
        "outputId": "4450887d-d1d6-4ebd-b6b5-c1115cb4a3fb"
      },
      "source": [
        "model_gb = GradientBoostingClassifier()\n",
        "model_gb.fit(X_train, y_train)\n",
        "preds_gb = model_gb.predict_proba(X_test)[:,1]\n",
        "print ('LogLoss score: {}'.format(str(log_loss(y_test, preds_gb))))\n",
        "print ('AUC score: {}'.format(str(roc_auc_score(y_test, preds_gb))))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogLoss score: 0.1656247228929607\n",
            "AUC score: 0.8176907076768725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6GBAnBpgk0_"
      },
      "source": [
        "pickle.dump(model_gb, open('gb.pkl', 'wb'))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArSiwODDr8R0",
        "outputId": "86d20910-a851-4e6b-d3ef-1c07ba0bf854"
      },
      "source": [
        "model_gb2 = GradientBoostingClassifier(max_depth=3, min_weight_fraction_leaf=0.001)\n",
        "model_gb2.fit(X_train, y_train)\n",
        "preds_gb2 = model_gb2.predict_proba(X_test)[:,1]\n",
        "print ('LogLoss score: {}'.format(str(log_loss(y_test, preds_gb2))))\n",
        "print ('AUC score: {}'.format(str(roc_auc_score(y_test, preds_gb2))))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogLoss score: 0.1658067883554617\n",
            "AUC score: 0.8163280936824684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFcZdW3Mgf3k"
      },
      "source": [
        "pickle.dump(model_gb2, open('gb2.pkl', 'wb'))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w04F7fBAox6g",
        "outputId": "87e31e92-60bc-4f5f-d0de-02582dffbab5"
      },
      "source": [
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "preds_lr = model_lr.predict_proba(X_test)[:,1]\n",
        "print ('LogLoss score: {}'.format(str(log_loss(y_test, preds_lr))))\n",
        "print ('AUC score: {}'.format(str(roc_auc_score(y_test, preds_lr))))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogLoss score: 0.17561467421598137\n",
            "AUC score: 0.7684700372074639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcP-fDYjbl5t"
      },
      "source": [
        "pickle.dump(model_lr, open('lr.pkl', 'wb'))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0bAgLKQu0rM",
        "outputId": "32333b49-7f56-43b8-8485-60516af12a81"
      },
      "source": [
        "model_rf = RandomForestClassifier(min_weight_fraction_leaf=0.0001, max_depth=20)\n",
        "model_rf.fit(X_train, y_train)\n",
        "preds_rf = model_rf.predict_proba(X_test)[:,1]\n",
        "print ('LogLoss score: {}'.format(str(log_loss(y_test, preds_rf))))\n",
        "print ('AUC score: {}'.format(str(roc_auc_score(y_test, preds_rf))))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogLoss score: 0.1646318095332512\n",
            "AUC score: 0.819229789699326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MANIV_I0bjAz"
      },
      "source": [
        "pickle.dump(model_rf, open('rf.pkl', 'wb'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNnluAdapor9",
        "outputId": "c0d6a5c9-0be4-4dec-935e-8ea552891803"
      },
      "source": [
        "model_knn = KNeighborsClassifier(n_neighbors=300)\n",
        "model_knn.fit(X_train, y_train)\n",
        "preds_knn = model_knn.predict_proba(X_test)[:,1]\n",
        "print ('LogLoss score: {}'.format(str(log_loss(y_test, preds_knn))))\n",
        "print ('AUC score: {}'.format(str(roc_auc_score(y_test, preds_knn))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogLoss score: 0.17299654552684413\n",
            "AUC score: 0.7850207966543266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wcPd11xZuAE"
      },
      "source": [
        "pickle.dump(model_knn, open('knn.pkl', 'wb'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQtaCB5upiFE"
      },
      "source": [
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M3STVrdprJe"
      },
      "source": [
        "def create_model_nn(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=input_dim))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(52))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy')\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoPtyVM0qK6x",
        "outputId": "e37c5ae8-1868-4d6d-c6da-bfd642f28562"
      },
      "source": [
        "model_nn = create_model_nn(X_train.shape[1])\n",
        "model_nn.fit(X_train, y_train, verbose=1, epochs=10, batch_size=1024)\n",
        "preds_nn = model_nn.predict(X_test)[:,0]\n",
        "print ('LogLoss score: {}'.format(str(log_loss(y_test, np.clip(preds_nn, a_min=10e-5, a_max = 1-10e-5)))))\n",
        "print ('AUC score: {}'.format(str(roc_auc_score(y_test, np.clip(preds_nn, a_min=10e-5, a_max = 1-10e-5)))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "810/810 [==============================] - 6s 3ms/step - loss: 0.1841\n",
            "Epoch 2/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1668\n",
            "Epoch 3/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1642\n",
            "Epoch 4/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1640\n",
            "Epoch 5/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1628\n",
            "Epoch 6/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1619\n",
            "Epoch 7/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1605\n",
            "Epoch 8/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1612\n",
            "Epoch 9/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1611\n",
            "Epoch 10/10\n",
            "810/810 [==============================] - 2s 3ms/step - loss: 0.1602\n",
            "LogLoss score: 0.168170889242463\n",
            "AUC score: 0.8151714673784262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUKQvQP5gp6Y"
      },
      "source": [
        "model_nn.save('nn.h5')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOLdd0bgp844"
      },
      "source": [
        "def create_model_nn2(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_dim=input_dim))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy')\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sanc0kIyqFzK",
        "outputId": "6e03510b-b774-4e67-f0c0-e1b1e3bd6cc1"
      },
      "source": [
        "model_nn2 = create_model_nn2(X_train.shape[1])\n",
        "model_nn2.fit(X_train, y_train, verbose=1, epochs=10, batch_size=1024)\n",
        "preds_nn2 = model_nn2.predict(X_test)[:,0]\n",
        "print ('LogLoss score: {}'.format(str(log_loss(y_test, np.clip(preds_nn2, a_min=10e-5, a_max = 1-10e-5)))))\n",
        "print ('AUC score: {}'.format(str(roc_auc_score(y_test, np.clip(preds_nn2, a_min=10e-5, a_max = 1-10e-5)))))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "810/810 [==============================] - 4s 4ms/step - loss: 0.1753\n",
            "Epoch 2/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1625\n",
            "Epoch 3/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1610\n",
            "Epoch 4/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1602\n",
            "Epoch 5/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1601\n",
            "Epoch 6/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1585\n",
            "Epoch 7/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1586\n",
            "Epoch 8/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1597\n",
            "Epoch 9/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1591\n",
            "Epoch 10/10\n",
            "810/810 [==============================] - 3s 4ms/step - loss: 0.1574\n",
            "LogLoss score: 0.16457861175969563\n",
            "AUC score: 0.8173687588194842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PUeHh2WhhKZ"
      },
      "source": [
        "model_nn2.save('nn2.h5')"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}